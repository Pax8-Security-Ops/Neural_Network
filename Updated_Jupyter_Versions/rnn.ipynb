{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8079991",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m trainingData \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAlexKing-Bailey\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCSV_Images\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSign-in_Activity.538-0700.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)                                               \u001b[38;5;66;03m# Assign the imported data to become the neural nets training data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m trainingData \u001b[38;5;241m=\u001b[39m trainingData\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues                                                 \u001b[38;5;66;03m# Extraction of values from the second column of the trainind data labeled with a 1 in their index\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m numpy\u001b[38;5;241m.\u001b[39mndarray                                                                                 \u001b[38;5;66;03m# Arranging the data into an array\u001b[39;00m\n\u001b[0;32m     19\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()                                                                       \u001b[38;5;66;03m# Activation of the scaler from sklearn and assigning it to a variable for further use\u001b[39;00m\n\u001b[0;32m     21\u001b[0m trainingData \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(trainingData\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))                               \u001b[38;5;66;03m# Using the scaler to reorganize the training data into a number range between -1 and 1\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler                                                # Scaling numerical features and differentiating them between 0 and 1 to make the machine learning model more flexible in terms of what kinds of data points it can process\n",
    "\n",
    "from tensorflow.keras.models import Sequential                                                # Making \"Sequential\" organization an available infrastructure with which to build the neural network\n",
    "from tensorflow.keras.layers import Dense                                                     # Simplifying the process of building layers that comprise the neural net for data to be processed\n",
    "from tensorflow.keras.layers import LSTM                                                      # Importing the LSTM tool library to make a Recurrent Neural Net\n",
    "from tensorflow.keras.layers import Dropout                                                   # Prevent the neural net from overfitting imported data and any fraction outputs are rounded down to zero\n",
    "\n",
    "trainingData = pd.read_csv(r'C:\\Users\\AlexKing-Bailey\\CSV_Images\\Sign-in_Activity.538-0700.csv')                                               # Assign the imported data to become the neural nets training data\n",
    "\n",
    "trainingData = trainingData.iloc[:, 1].values                                                 # Extraction of values from the second column of the trainind data labeled with a 1 in their index\n",
    "\n",
    "numpy.ndarray                                                                                 # Arranging the data into an array\n",
    "\n",
    "scaler = MinMaxScaler()                                                                       # Activation of the scaler from sklearn and assigning it to a variable for further use\n",
    "\n",
    "trainingData = scaler.fit_transform(trainingData.reshape(-1,1))                               # Using the scaler to reorganize the training data into a number range between -1 and 1\n",
    "\n",
    "# Two empty lists with which to divide the overall training data frame \n",
    "X_trainingData = []                                                                           # x = data input sequences to feed the neural net\n",
    "Y_trainingData = []                                                                           # y = output sequences following  data processing through the neural net\n",
    "\n",
    "# Arrangement of data feautres for time-series data\n",
    "for i in range(40, len(trainingData)):\n",
    "  X_trainingData.append(trainingData[i-40:i, 0])                                              # Extract 40 data points to act as input for the neural net\n",
    "  Y_trainingData.append(training[i, 0])                                                       # Extract each output devised by the neural net\n",
    "\n",
    "X_trainingData = np.array(X_trainingData)                                                     # Arrange the input sequences into an array\n",
    "Y_trainingData = np.array(Y_trainingData)                                                     # Arrange the output sequences into an array \n",
    "\n",
    "# Verifying the shape of the arrays\n",
    "print(X_trainingData.shape) \n",
    "print(Y_trainingData.shape)\n",
    "\n",
    "# Arranging the arrays into a binary classification to be compatiable with Tensorflow Tools\n",
    "X_trainingData = np.reshape(X_trainingData, (X_trainingData.shape[0],\n",
    "                                             X_trainingData.shape[1],\n",
    "                                             1))\n",
    "\n",
    "print(X_trainingData.shape)                                                                    # Verification of new input sequence organization\n",
    "rnn = Sequential()                                                                             # Activation function for the sequential neural net\n",
    "\n",
    "rnn.add(LSTM(units = 45, return_sequences = True, input_shape = (X_trainingData.shape[1], 1))) # Layer one of the LSTM layer\n",
    "\n",
    "rnn.add(Dropout(0.2))                                                                          # Perform dropout at regular intervals\n",
    "\n",
    "for i in [True, True, False]:                                                                  # Adding three more layers in a for loop, accompanied by dropout commands\n",
    "  rnn.add(LSTM(units = 45, return_sequences = i))\n",
    "  rnn.add(Dropout(0.2))\n",
    "\n",
    "rnn.add(Dense(units = 1)) # Adding an output layer \n",
    "\n",
    "# Compiling all data in the neural net\n",
    "rnn.compile(optimizer = '', loss = '')\n",
    "rnn.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "rnn.fit(X_trainingData, Y_trainingData, epochs = 100, batch_size = 32)                         # Activation function for the actual neural net\n",
    "\n",
    "testData = pd.read_csv('testDatafile.csv')                                                     # Importing the test data\n",
    "testData = testData.iloc[:, 1].values                                                          # Converting the data into a numPy array\n",
    "\n",
    "print(testData.shape)                                                                          # Verify the importation and structure of the test data\n",
    "\n",
    "plt.plot(testData)                                                                             # Visualize the test data\n",
    "\n",
    "unscaledTrainingdata = pd.read_csv('Sign-in_Activity.538-0700.csv')                                        # Creating a version of the training data not converted into a numpy array\n",
    "unscaledTestdata = pd.read_csv('testDatafile.csv')                                             # Creating a version of the testing data not converted into a numpy array\n",
    "allData = pd.concat((unscaled_X_trainingData['Open'], unscaledTestdata['Open']), axis = 0)     # Concatenate both unscaled data sets into a single variable \n",
    "\n",
    "X_testData = allData[len(allData) - len(testData) - 40:].values                                 # Calculation of data by subtraction of the amount of test data starting from position 40 and the overall length of the concatenated data sets\n",
    "X_testData = np.reshape(X_testData, (-1,1))                                                    # Convert the difference to number range between -1 and 1\n",
    "X_testData = scalar.transform(X_testData)                                                      # Scale the testing data\n",
    "\n",
    "final_X_testData = []                                                                          # Collected test data goes into a single list\n",
    "for i in range (40, len(X_testData)):                                                          \n",
    "  final_X_testData.append(X_testData[i-40:i, 0])\n",
    "\n",
    "final_X_testData = np.array(final_X_testData)                                                  # Arranging the testing data into an array\n",
    "\n",
    "# Arranging the test data array into a tensorflow-compatiable format\n",
    "final_X_testData = np.reshape(final_X_testData, (final_X_testData.shape[0],\n",
    "                                                 final_X_testData.shape[1],\n",
    "                                                 1))\n",
    "\n",
    "predictions = rnn.predict(final_X_testData)                                                    # Generation of predicted values following the neural nets activation and processing of data \n",
    "\n",
    "unscaledPredictions = scaler.inverseTransform(predictions)                                     # Unscaling predicted values and re-plotting the data\n",
    "plt.clf()                                                                                      \n",
    "plt.plot(unscaledPredictions)\n",
    "\n",
    "# Visualizing the education of the neural net and displaying it's conclusions\n",
    "plt.plot(unscaledPredictions, color = '#135485', label = \"Predictions\")\n",
    "plt.plot(testData, color = 'black', label = \"Read Data\")\n",
    "plt.title (\"Machine Learning Prediction Educaton\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f7f8e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (1612599018.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(Sign-in_Activity.538-0700.csv)\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "print(Sign-in_Activity.538-0700.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c58acf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unscaledTrainingData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m unscaledTrainingData\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unscaledTrainingData' is not defined"
     ]
    }
   ],
   "source": [
    "unscaledTrainingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a394c85e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
